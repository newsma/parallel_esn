

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Echo State Network Training &mdash; Parallel Echo State Network 0.post43+gc7e2a7d documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script>
        <script type="text/javascript" src="_static/copybutton.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Release History" href="release-history.html" />
    <link rel="prev" title="Usage" href="usage.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> Parallel Echo State Network
          

          
          </a>

          
            
            
              <div class="version">
                0.post43+gc7e2a7d
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage.html">Usage</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Echo State Network Training</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#defining-the-network">Defining the Network</a></li>
<li class="toctree-l2"><a class="reference internal" href="#function-documentation">Function Documentation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="release-history.html">Release History</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Parallel Echo State Network</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Echo State Network Training</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/network-training.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="echo-state-network-training">
<h1>Echo State Network Training<a class="headerlink" href="#echo-state-network-training" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>The primary interface with the echo state network (ESN) in this code base is
through the <code class="xref py py-func docutils literal notranslate"><span class="pre">ESN()</span></code> class. This class allows one to define an
ESN and fit it to data with a given set of hyper-parameters.</p>
</div>
<div class="section" id="defining-the-network">
<h2>Defining the Network<a class="headerlink" href="#defining-the-network" title="Permalink to this headline">¶</a></h2>
<p>The usual format one uses when defining an ESN is building a sparse matrix
with a specified spectral radius, <span class="math notranslate nohighlight">\(\rho\)</span>, that follows either a uniform
or a normal distribution for the weights. However, we have opted to define
the reservoir of the ESN as a small-world network. We did this because there
has been recent research which suggests that by having a more fine-grained
specification of the reservoir network, one can see improvements in
out-of-sample performance for the model <a class="reference internal" href="#kawai2017echo" id="id1">[KTPA17]</a>.</p>
<p>Thus instead of specifying the sparsity of the reservoir, a user can now have
greater control over the network by providing the number of neighbors and the
re-wiring probability for the graph.</p>
</div>
<div class="section" id="function-documentation">
<h2>Function Documentation<a class="headerlink" href="#function-documentation" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="parallel_esn.esn.ESN">
<em class="property">class </em><code class="descclassname">parallel_esn.esn.</code><code class="descname">ESN</code><span class="sig-paren">(</span><em>input_dim</em>, <em>hidden_dim</em>, <em>output_dim</em>, <em>k</em>, <em>spectral_radius=0.9</em>, <em>p=0.1</em>, <em>beta=0.001</em>, <em>alpha=0.7</em>, <em>random_state=None</em>, <em>weight_distn='uniform'</em>, <em>use_cython=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/parallel_esn/esn.html#ESN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#parallel_esn.esn.ESN" title="Permalink to this definition">¶</a></dt>
<dd><p>Sequential Echo State Network class</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Size of input dimension, N_u</p></li>
<li><p><strong>hidden_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of hidden units in the W matrix, N_x</p></li>
<li><p><strong>output_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Dimensionality of the output, N_y</p></li>
<li><p><strong>k</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – k-nearest neighbors each node is connected to in the small-world
network for the hidden layer</p></li>
<li><p><strong>spectral_radius</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Spectral radius of the reservoir</p></li>
<li><p><strong>p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Re-wiring probability for small-world network</p></li>
<li><p><strong>beta</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Regularization parameter for L2 regression</p></li>
<li><p><strong>alpha</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – ESN leaking rate</p></li>
<li><p><strong>random_state</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>np.random.RandomState</em><em>, </em><em>optional</em>) – Random state initializer</p></li>
<li><p><strong>weight_distn</strong> (<em>{&quot;uniform&quot;</em><em>, </em><em>&quot;normal&quot;}</em><em>, </em><em>optional</em>) – Distribution of reservoir weights</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="parallel_esn.esn.ESN.clear_state">
<code class="descname">clear_state</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/parallel_esn/esn.html#ESN.clear_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#parallel_esn.esn.ESN.clear_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Clears X0, the reservoir’s memory of previous inputs and neural state</p>
</dd></dl>

<dl class="method">
<dt id="parallel_esn.esn.ESN.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>U</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/parallel_esn/esn.html#ESN.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#parallel_esn.esn.ESN.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts Yhat, output observations, given time series of inputs U</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>U</strong> (<em>np.ndarray</em>) – Input data array, columns u(n) concatenated horizontally.
Dimensions - N_u x T</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>Yhat</strong> – Prediction of observations. Returns feature vectors as columns stacked horizontally
in time. Take the transpose of this output to
obtain feature vectors as rows stacked vertically in time.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="parallel_esn.esn.ESN.predict_with_X">
<code class="descname">predict_with_X</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/parallel_esn/esn.html#ESN.predict_with_X"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#parallel_esn.esn.ESN.predict_with_X" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts Yhat, output observations, given X already generated
from input data U. Useful if X has already been computed and a prediction
is desired without affecting the current state of the reservoir.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>np.ndarray</em>) – X is [1;u(n);x(n)] concatenated horizontally (n is time), generated
from the input data U.
Dimensions of (1+ N_u + N_x) x T</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>Yhat</strong> – Prediction of observations.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="parallel_esn.esn.ESN.recursive_predict">
<code class="descname">recursive_predict</code><span class="sig-paren">(</span><em>U</em>, <em>iterations</em>, <em>cold_start=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/parallel_esn/esn.html#ESN.recursive_predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#parallel_esn.esn.ESN.recursive_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicts Yhat, output observations following the given time series of inputs U
This method assumes that the network has been trained to produce one-step-forecasting,
where the output y(t) corresponds to u(t+1), i.e. what the next input would have been.
Currently this method only supports predicting all features provided as input; the
dimensions of vector y(t) must match the dimensions of u(t).</p>
<p>For the first step, observed values u(t) for t=0..T-1 are used to produce the first
predicted value y(t) = hat{u}(t+1), which is then fed back to the network as an
input in order to produce y(t+1). This recursion is continued for the specified
number of iterations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>U</strong> (<em>np.ndarray</em>) – Input data array, columns u(n) concatenated horizontally.
Dimensions - N_u x T</p></li>
<li><p><strong>iterations</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – How many future times to predict.</p></li>
<li><p><strong>cold_start</strong> (<em>boolean</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – Whether to clear reservoir state before driving the reservoir with input data U.
If the input data follows directly after training data, a warm start is sensible.
However, if the provided data is temporally disconnected from the training data,
a cold start could be useful for reproducibility if this method will be called
multiple times, on the same data or on other inputs.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>Yhat</strong> – Prediction of observations. Returns feature vectors as columns stacked horizontally
in time. Take the transpose of this output to
obtain feature vectors as rows stacked vertically in time.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="parallel_esn.esn.ESN.recursive_score">
<code class="descname">recursive_score</code><span class="sig-paren">(</span><em>U</em>, <em>Y_true</em>, <em>input_len</em>, <em>pred_len</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/parallel_esn/esn.html#ESN.recursive_score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#parallel_esn.esn.ESN.recursive_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes loss in recursive one-step prediction, intended for validation set.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>U</strong> (<em>np.ndarray</em>) – Input data array, columns u(n) concatenated horizontally.
Dimensions - N_u x T</p></li>
<li><p><strong>Y_true</strong> (<em>np.ndarray</em>) – Target output array,  y(n) concatenated horizontally in time.
Dimensions - N_y x T</p></li>
<li><p><strong>input_len</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – The input length to be fed to the ESN before recursive single-step prediction.</p></li>
<li><p><strong>pred_len</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – The number of predictions desired.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>error</strong> – Normalized mean square error (NMSE). On the prediction. Each feature’s
NMSE is computed separately and averaged together at the end.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)">float</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="parallel_esn.esn.ESN.recursive_train_validate">
<code class="descname">recursive_train_validate</code><span class="sig-paren">(</span><em>trainU</em>, <em>trainY</em>, <em>valU</em>, <em>valY</em>, <em>input_len</em>, <em>pred_len</em>, <em>warmup=10</em>, <em>verbose=1</em>, <em>compute_loss_freq=-1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/parallel_esn/esn.html#ESN.recursive_train_validate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#parallel_esn.esn.ESN.recursive_train_validate" title="Permalink to this definition">¶</a></dt>
<dd><p>Train on provided training data, and immediately validate
and return validation loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>trainU</strong> (<em>array_like of np.ndarray</em>) – Batch of training input data arrays, columns u(n) concatenated horizontally.
Dimensions - Batch_size x N_x x T_i</p></li>
<li><p><strong>trainY</strong> (<em>array_like of np.ndarray</em>) – Batch of training true output data arrays.
Dimensions - Batch_size x N_y x T_i</p></li>
<li><p><strong>valU</strong> (<em>array_like of np.ndarray</em>) – Batch of validation input data arrays, columns u(n) concatenated horizontally.
Dimensions - Batch_size x N_x x T_i</p></li>
<li><p><strong>valY</strong> (<em>array_like of np.ndarray</em>) – Batch of validation true output data arrays.
Dimensions - Batch_size x N_y x T_i</p></li>
<li><p><strong>input_len</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – The input length to be fed to the ESN before recursive single-step prediction.</p></li>
<li><p><strong>pred_len</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – The number of predictions desired.</p></li>
<li><p><strong>warmup</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em><em>, </em><em>default=10</em>) – The number of states to discard at the beginning of each train/validation batch,
before initial transients in the reservoir have died out. The amount to discard
depends on the memory of the network and typically ranges from 10s to 100s.</p></li>
<li><p><strong>verbose</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Whether to print status of training</p></li>
<li><p><strong>compute_loss_freq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em><em>, </em><em>default=-1</em>) – How often to compute training loss. Only for information, not necessary
for training. Negative value disables computing training loss.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>loss</strong> – Returns the sum of the losses computed on each sequence in validation set.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)">float</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="parallel_esn.esn.ESN.recursive_validate">
<code class="descname">recursive_validate</code><span class="sig-paren">(</span><em>batchU</em>, <em>batchY_true</em>, <em>input_len</em>, <em>pred_len</em>, <em>verbose=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/parallel_esn/esn.html#ESN.recursive_validate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#parallel_esn.esn.ESN.recursive_validate" title="Permalink to this definition">¶</a></dt>
<dd><p>Get loss on validation set for recursive one-step prediction. Uses recursive_score
to compute the total error.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batchU</strong> (<em>array_like of np.ndarray</em>) – Batch of input data arrays, columns u(n) concatenated horizontally
Dimensions - Batch_size x N_x x T_i</p></li>
<li><p><strong>batchY_true</strong> (<em>array_like of np.ndarray</em>) – Batch of true output data arrays
Dimensions - Batch_size x N_y x T_i</p></li>
<li><p><strong>input_len</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – The input length to be fed to the ESN before recursive single-step prediction.</p></li>
<li><p><strong>pred_len</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – The number of predictions desired.</p></li>
<li><p><strong>verbose</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Whether to print status of training</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>loss</strong> – Returns the average of the NMSE losses computed on each sequence</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)">float</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="parallel_esn.esn.ESN.reset">
<code class="descname">reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/parallel_esn/esn.html#ESN.reset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#parallel_esn.esn.ESN.reset" title="Permalink to this definition">¶</a></dt>
<dd><p>Reset output layer training and clear state</p>
</dd></dl>

<dl class="method">
<dt id="parallel_esn.esn.ESN.score">
<code class="descname">score</code><span class="sig-paren">(</span><em>U</em>, <em>Y_true</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/parallel_esn/esn.html#ESN.score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#parallel_esn.esn.ESN.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes loss</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>U</strong> (<em>np.ndarray</em>) – Input data array, columns u(n) concatenated horizontally.
Dimensions - N_u x T</p></li>
<li><p><strong>Y_true</strong> (<em>np.ndarray</em>) – Target output array,  y(n) concatenated horizontally in time.
Dimensions - N_y x T</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>error</strong> – Normalized mean square error (NMSE). Each feature’s NMSE is
computed separately and averaged together at the end.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)">float</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="parallel_esn.esn.ESN.score_with_X">
<code class="descname">score_with_X</code><span class="sig-paren">(</span><em>X</em>, <em>Y_true</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/parallel_esn/esn.html#ESN.score_with_X"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#parallel_esn.esn.ESN.score_with_X" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes loss given input data X already processed with reservoir
activations. Functions identically to running:</p>
<p>self.score(self._compute_X(U), Y_true)</p>
<p>provided X corresponds to U, and if starting from the same reservoir
initial state.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>np.ndarray</em>) – X is [1;u(n);x(n)] concatenated horizontally (n is time), generated
from the input data U.
Dimensions of (1+ N_u + N_x) x T</p></li>
<li><p><strong>Y_true</strong> (<em>np.ndarray</em>) – Target output array,  y(n) concatenated horizontally in time.
Dimensions - N_y x T</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>error</strong> – Normalized mean square error (NMSE). Each feature’s NRMSE is
computed separately and averaged together at the end.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)">float</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="parallel_esn.esn.ESN.train">
<code class="descname">train</code><span class="sig-paren">(</span><em>batchU</em>, <em>batchY_true</em>, <em>clear_state=False</em>, <em>warmup=10</em>, <em>verbose=1</em>, <em>compute_loss_freq=-1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/parallel_esn/esn.html#ESN.train"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#parallel_esn.esn.ESN.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Trains on U’s and corresponding Y_true’s, batched in first index.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batchU</strong> (<em>array_like of np.ndarray</em>) – Batch of input data arrays, columns u(n) concatenated horizontally
Dimensions - Batch_size x N_x x T_i</p></li>
<li><p><strong>batchY_true</strong> (<em>array_like of np.ndarray</em>) – batch of true output data arrays
Dimensions - Batch_size x N_y x T_i</p></li>
<li><p><strong>clear_state</strong> (<em>boolean</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – Whether to clear the reservoir memory in between batches. If False, the training
on the batches is equivalent to if the batches were all concatenated into a single
time series.</p></li>
<li><p><strong>warmup</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em><em>, </em><em>default=10</em>) – The number of states to discard at the beginning of training, before initial
transients in the reservoir have died out. The amount to discard depends on
the memory of the network and typically ranges from 10s to 100s. If batches are
to be treated as independent, with clear_state=True, warmups can typically be shorter
since the zeroed reservoir initialization would be the normal operating mode of
the ESN.</p></li>
<li><p><strong>verbose</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Whether to print status of training</p></li>
<li><p><strong>compute_loss_freq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>default=-1</em>) – How often to compute training loss. Only for information, not necessary
for training. Negative value disables computing training loss.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>loss</strong> – Returns the loss computed on each sequence where loss was computed, array
of length ((Batch_size-1) // compute_loss_freq) + 1. None returned if
compute_loss_freq is less than equal to 0.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="parallel_esn.esn.ESN.train_validate">
<code class="descname">train_validate</code><span class="sig-paren">(</span><em>trainU</em>, <em>trainY</em>, <em>valU</em>, <em>valY</em>, <em>warmup=10</em>, <em>verbose=1</em>, <em>compute_loss_freq=-1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/parallel_esn/esn.html#ESN.train_validate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#parallel_esn.esn.ESN.train_validate" title="Permalink to this definition">¶</a></dt>
<dd><p>Train on provided training data, and immediately validate
and return validation loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>trainU</strong> (<em>array_like of np.ndarray</em>) – Batch of training input data arrays, columns u(n) concatenated horizontally.
Dimensions - Batch_size x N_x x T_i</p></li>
<li><p><strong>trainY</strong> (<em>array_like of np.ndarray</em>) – Batch of training true output data arrays.
Dimensions - Batch_size x N_y x T_i</p></li>
<li><p><strong>valU</strong> (<em>array_like of np.ndarray</em>) – Batch of validation input data arrays, columns u(n) concatenated horizontally.
Dimensions - Batch_size x N_x x T_i</p></li>
<li><p><strong>valY</strong> (<em>array_like of np.ndarray</em>) – Batch of validation true output data arrays.
Dimensions - Batch_size x N_y x T_i</p></li>
<li><p><strong>warmup</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em><em>, </em><em>default=10</em>) – The number of states to discard at the beginning of each train/validation batch,
before initial transients in the reservoir have died out. The amount to discard
depends on the memory of the network and typically ranges from 10s to 100s.</p></li>
<li><p><strong>verbose</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Whether to print status of training</p></li>
<li><p><strong>compute_loss_freq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em><em>, </em><em>default=-1</em>) – How often to compute training loss. Only for information, not necessary
for training. Negative value disables computing training loss.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>loss</strong> – Returns the sum of the losses computed on each sequence in validation set.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)">float</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="parallel_esn.esn.ESN.validate">
<code class="descname">validate</code><span class="sig-paren">(</span><em>batchU</em>, <em>batchY_true</em>, <em>warmup=10</em>, <em>verbose=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/parallel_esn/esn.html#ESN.validate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#parallel_esn.esn.ESN.validate" title="Permalink to this definition">¶</a></dt>
<dd><p>Get loss on validation set, given past sequences in batchU and observed outcomes batchY_true</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batchU</strong> (<em>array_like of np.ndarray</em>) – Batch of input data arrays, columns u(n) concatenated horizontally
Dimensions - Batch_size x N_x x T_i</p></li>
<li><p><strong>batchY_true</strong> (<em>array_like of np.ndarray</em>) – Batch of true output data arrays
Dimensions - Batch_size x N_y x T_i</p></li>
<li><p><strong>warmup</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em><em>, </em><em>default=10</em>) – The number of states to discard at the beginning of each validation batch, before initial
transients in the reservoir have died out. The amount to discard depends on
the memory of the network and typically ranges from 10s to 100s.</p></li>
<li><p><strong>verbose</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Whether to print status of training</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>loss</strong> – Returns the average of the NMSE losses computed on each sequence</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)">float</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<p id="bibtex-bibliography-network-training-0"><dl class="citation">
<dt class="label" id="kawai2017echo"><span class="brackets"><a class="fn-backref" href="#id1">KTPA17</a></span></dt>
<dd><p>Yuji Kawai, Tatsuya Tokuno, Jihoon Park, and Minoru Asada. Echo in a small-world reservoir: time-series prediction using an economical recurrent neural network. In <em>2017 Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)</em>, 126–131. IEEE, 2017.</p>
</dd>
</dl>
</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="release-history.html" class="btn btn-neutral float-right" title="Release History" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="usage.html" class="btn btn-neutral float-left" title="Usage" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Zachary Blanks

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>